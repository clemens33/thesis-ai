
@article{DeepTox,
  title={{DeepTox: Toxicity Prediction using Deep Learning}},
  author={Mayr, Andreas and Klambauer, G{\"u}nter and Unterthiner, Thomas and Hochreiter, Sepp},
  journal={Frontiers in Environmental Science},
  volume={3},
  year={2016},
  number={80}
}

@software{saabas_treeinterpreter_2021,
	title = {{TreeInterpreter}},
	rights = {{BSD}-3-Clause},
	url = {https://github.com/andosa/treeinterpreter},
	author = {Saabas, Ando},
	urldate = {2021-12-05},
	date = {2021-12-03},
	note = {original-date: 2015-08-02T20:26:21Z},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{kokhlikyan2020captum,
    title={Captum: A unified and generic model interpretability library for PyTorch},
    author={Narine Kokhlikyan and Vivek Miglani and Miguel Martin and Edward Wang and Bilal Alsallakh and Jonathan Reynolds and Alexander Melnikov and Natalia Kliushkina and Carlos Araya and Siqi Yan and Orion Reblitz-Richardson},
    year={2020},
    eprint={2009.07896},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{schimunek_poster_2021,
    title={Poster: Comparative assessment of interpretability methods of deep activity models for hERG},
    author={Johannes Schimunek and Lukas Friedrich and Daniel Kuhn and Sepp Hochreiter and Günter Klambauer},
    date = {2021-06},
    year={2021},
    note = {In 19th International Workshop on (Quantitative) Structure-Activity Relationships in Environmental and Health Sciences}
}

@article{jiang_could_2021,
	title = {Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models},
	volume = {13},
	issn = {1758-2946},
	url = {https://doi.org/10.1186/s13321-020-00479-8},
	doi = {10.1186/s13321-020-00479-8},
	shorttitle = {Could graph neural networks learn better molecular representation for drug discovery?},
	abstract = {Graph neural networks ({GNN}) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that {GNN} could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning ({ML}) algorithms, including four descriptor-based models ({SVM}, {XGBoost}, {RF} and {DNN}) and four graph-based models ({GCN}, {GAT}, {MPNN} and Attentive {FP}), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. {SVM} generally achieves the best predictions for the regression tasks. Both {RF} and {XGBoost} can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive {FP} and {GCN}, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, {XGBoost} and {RF} are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the {SHAP} method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening ({VS}) towards {HIV} and demonstrated that different {ML} algorithms offer diverse {VS} profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.},
	pages = {12},
	number = {1},
	journaltitle = {Journal of Cheminformatics},
	shortjournal = {Journal of Cheminformatics},
	author = {Jiang, Dejun and Wu, Zhenxing and Hsieh, Chang-Yu and Chen, Guangyong and Liao, Ben and Wang, Zhe and Shen, Chao and Cao, Dongsheng and Wu, Jian and Hou, Tingjun},
	urldate = {2021-11-27},
	date = {2021-02-17},
	keywords = {{ADME}/T prediction, Deep learning, Ensemble learning, Extreme gradient boosting, Graph neural networks},
	file = {Full Text PDF:/home/ck/Zotero/storage/QMLJ26H7/Jiang et al. - 2021 - Could graph neural networks learn better molecular.pdf:application/pdf;Snapshot:/home/ck/Zotero/storage/KTRDDHW7/s13321-020-00479-8.html:text/html},
}

@article{ramsauer_hopfield_2020,
	title = {Hopfield Networks is All You Need},
	url = {http://arxiv.org/abs/2008.02217},
	abstract = {We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the {UCI} benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: https://github.com/ml-jku/hopfield-layers},
	journaltitle = {{arXiv}:2008.02217 [cs, stat]},
	author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Adler, Thomas and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
	urldate = {2021-04-06},
	date = {2020-12-22},
	eprinttype = {arxiv},
	eprint = {2008.02217},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/AL4QVB8U/Ramsauer et al. - 2020 - Hopfield Networks is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/ATAG5MPL/2008.html:text/html},
}