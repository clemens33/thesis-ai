

@software{baosenguo_kaggle-moa_2021,
	title = {Kaggle-{MoA} 2nd Place Solution},
	url = {https://github.com/baosenguo/Kaggle-MoA-2nd-Place-Solution},
	author = {baosenguo},
	urldate = {2021-10-13},
	date = {2021-10-12},
	note = {original-date: 2020-12-09T02:24:45Z},
}

@online{alessandro_mechanism_nodate,
	title = {Mechanism of Action ({MoA}) Prediction - Kaggle Competition},
	url={https://www.epfl.ch/labs/mlo/wp-content/uploads/2021/05/crpmlcourse-paper839.pdf},
	urldate = {2021-10-13},
	abstract = {Drugs research is rapidly evolving and latest progress is partially thanks to the introduction of Machine Learning tools. A variety of machine learning methods such as naive Bayesian, support vector machine and, more recently, deep neural networks are demonstrating their usefulness in drug discovery and development. The aim of this paper is to propose various Machine Learning algorithms in order to predict the biological activity of a molecule, which is referred to as mechanismof-action, or {MoA} for short. The model selection has been carried out through a online public competition that elected the best performing models for the given dataset. We describe here the models, based on the Neural Network framework, that we utilized and implemented during the challenge and that made us achieve the top 2\% of the ﬁnal leaderboard, with a position of 72 over 4373 teams.},	
	author = {Alessandro, Lombardi and Niccolo’, Polvani and Filippo, Zacchei and Fabrice, Krapp Lucien and Matteo, Dal Peraro},
	langid = {english},
	file = {Alessandro et al. - Mechanism of Action (MoA) Prediction - Kaggle Comp.pdf:/home/ck/Zotero/storage/TKP9U4WI/Alessandro et al. - Mechanism of Action (MoA) Prediction - Kaggle Comp.pdf:application/pdf},
}

@online{c_kaggle2020,
	title = {State of Data Science and Machine Learning 2020},
	url = {https://www.kaggle.com/kaggle-survey-2020},
	abstract = {Download our executive summary for a profile of today's working data scientist and their tools},
	urldate = {2021-10-11},
	langid = {english},
	file = {Snapshot:/home/ck/Zotero/storage/M4J3N3KF/kaggle-survey-2020.html:text/html},
}


@online{c_amount_unstructureddata,
	title = {Structured vs Unstructured Data 101: Top Guide},
	url = {https://www.datamation.com/big-data/structured-vs-unstructured-data/},
	shorttitle = {Structured vs Unstructured Data 101},
	abstract = {Structured Data is organized \& often formatted, and Unstructured Data is raw data of various types. Learn key differences \& how each is used.},
	titleaddon = {Datamation},
	urldate = {2021-10-11},
	date = {2021-05-21},
	langid = {american},
	file = {Snapshot:/home/ck/Zotero/storage/TS5CTUTL/structured-vs-unstructured-data.html:text/html},
}



@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	% journaltitle = {{arXiv}:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2021-10-11},
	date = {2015-12-10},
	eprinttype = {arxiv},
	eprint = {1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/5SVWGLHC/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/W4G5NIA5/1512.html:text/html},
}


@article{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2021-10-11},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/FRZBXYM2/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}


@online{c_struc_vs_unstruc,
	title = {Deep Learning with Structured Data},
	url = {https://www.manning.com/books/deep-learning-with-structured-data},
	abstract = {Deep learning offers the potential to identify complex patterns and relationships hidden in data of all sorts. Deep Learning with Structured Data{\textless}/i{\textgreater} shows you how to apply powerful deep learning analysis techniques to the kind of structured, tabular data you'll find in the relational databases that real-world businesses depend on. Filled with practical, relevant applications, this book teaches you how deep learning can augment your existing machine learning and business intelligence systems.},
	titleaddon = {Manning Publications},
	urldate = {2021-10-11},
	langid = {english},
	file = {Snapshot:/home/ck/Zotero/storage/8X8WQMX9/deep-learning-with-structured-data.html:text/html},
}


@article{shwartz-ziv_tabular_2021,
	title = {Tabular Data: Deep Learning is Not All You Need},
	url = {http://arxiv.org/abs/2106.03253},
	shorttitle = {Tabular Data},
	abstract = {A key element of {AutoML} systems is setting the types of models that will be used for each type of task. For classification and regression problems with tabular data, the use of tree ensemble models (like {XGBoost}) is usually recommended. However, several deep learning models for tabular data have recently been proposed, claiming to outperform {XGBoost} for some use-cases. In this paper, we explore whether these deep models should be a recommended option for tabular data, by rigorously comparing the new deep models to {XGBoost} on a variety of datasets. In addition to systematically comparing their accuracy, we consider the tuning and computation they require. Our study shows that {XGBoost} outperforms these deep models across the datasets, including datasets used in the papers that proposed the deep models. We also demonstrate that {XGBoost} requires much less tuning. On the positive side, we show that an ensemble of the deep models and {XGBoost} performs better on these datasets than {XGBoost} alone.},
	journaltitle = {{arXiv}:2106.03253 [cs]},
	author = {Shwartz-Ziv, Ravid and Armon, Amitai},
	urldate = {2021-10-11},
	date = {2021-06-06},
	eprinttype = {arxiv},
	eprint = {2106.03253},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/HMG872QA/Shwartz-Ziv and Armon - 2021 - Tabular Data Deep Learning is Not All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/UE4QAJ6M/2106.html:text/html},
}


@article{kadra_regularization_2021,
	title = {Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data},
	url = {http://arxiv.org/abs/2106.11189},
	shorttitle = {Regularization is all you Need},
	abstract = {Tabular datasets are the last "unconquered castle" for deep learning, with traditional {ML} methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron ({MLP}) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for {MLPs} on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain {MLPs} significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional {ML} methods, such as {XGBoost}.},
	journaltitle = {{arXiv}:2106.11189 [cs]},
	author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
	urldate = {2021-10-11},
	date = {2021-06-21},
	eprinttype = {arxiv},
	eprint = {2106.11189},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/JGY28PF6/Kadra et al. - 2021 - Regularization is all you Need Simple Neural Nets.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/GMJ6GIJM/2106.html:text/html},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}


@article{ke_tabnn_2018,
	title = {{TabNN}: A Universal Neural Network Solution for Tabular Data},
	url = {https://openreview.net/forum?id=r1eJssCqY7},
	shorttitle = {{TabNN}},
	abstract = {We propose a universal neural network solution to derive effective {NN} architectures for tabular data automatically.},
	author = {Ke, Guolin and Zhang, Jia and Xu, Zhenhui and Bian, Jiang and Liu, Tie-Yan},
	urldate = {2021-10-12},
	date = {2018-09-27},
	langid = {english},
	file = {Full Text PDF:/home/ck/Zotero/storage/XX3XANRT/Ke et al. - 2018 - TabNN A Universal Neural Network Solution for Tab.pdf:application/pdf;Snapshot:/home/ck/Zotero/storage/YKKVLLBI/forum.html:text/html},
}


@article{jiang_could_2021,
	title = {Could graph neural networks learn better molecular representation for drug discovery? A comparison study of descriptor-based and graph-based models},
	volume = {13},
	issn = {1758-2946},
	url = {https://doi.org/10.1186/s13321-020-00479-8},
	doi = {10.1186/s13321-020-00479-8},
	shorttitle = {Could graph neural networks learn better molecular representation for drug discovery?},
	abstract = {Graph neural networks ({GNN}) has been considered as an attractive modelling method for molecular property prediction, and numerous studies have shown that {GNN} could yield more promising results than traditional descriptor-based methods. In this study, based on 11 public datasets covering various property endpoints, the predictive capacity and computational efficiency of the prediction models developed by eight machine learning ({ML}) algorithms, including four descriptor-based models ({SVM}, {XGBoost}, {RF} and {DNN}) and four graph-based models ({GCN}, {GAT}, {MPNN} and Attentive {FP}), were extensively tested and compared. The results demonstrate that on average the descriptor-based models outperform the graph-based models in terms of prediction accuracy and computational efficiency. {SVM} generally achieves the best predictions for the regression tasks. Both {RF} and {XGBoost} can achieve reliable predictions for the classification tasks, and some of the graph-based models, such as Attentive {FP} and {GCN}, can yield outstanding performance for a fraction of larger or multi-task datasets. In terms of computational cost, {XGBoost} and {RF} are the two most efficient algorithms and only need a few seconds to train a model even for a large dataset. The model interpretations by the {SHAP} method can effectively explore the established domain knowledge for the descriptor-based models. Finally, we explored use of these models for virtual screening ({VS}) towards {HIV} and demonstrated that different {ML} algorithms offer diverse {VS} profiles. All in all, we believe that the off-the-shelf descriptor-based models still can be directly employed to accurately predict various chemical endpoints with excellent computability and interpretability.},
	pages = {12},
	number = {1},
	journaltitle = {Journal of Cheminformatics},
	shortjournal = {Journal of Cheminformatics},
	author = {Jiang, Dejun and Wu, Zhenxing and Hsieh, Chang-Yu and Chen, Guangyong and Liao, Ben and Wang, Zhe and Shen, Chao and Cao, Dongsheng and Wu, Jian and Hou, Tingjun},
	urldate = {2021-10-11},
	date = {2021-02-17},
	keywords = {{ADME}/T prediction, Deep learning, Ensemble learning, Extreme gradient boosting, Graph neural networks},
	file = {Full Text PDF:/home/ck/Zotero/storage/44IV6H3A/Jiang et al. - 2021 - Could graph neural networks learn better molecular.pdf:application/pdf},
}

@article{arik_tabnet_2020,
	title = {{TabNet}: Attentive Interpretable Tabular Learning},
	url = {http://arxiv.org/abs/1908.07442},
	shorttitle = {{TabNet}},
	abstract = {We propose a novel high-performance and interpretable canonical deep tabular data learning architecture, {TabNet}. {TabNet} uses sequential attention to choose which features to reason from at each decision step, enabling interpretability and more efficient learning as the learning capacity is used for the most salient features. We demonstrate that {TabNet} outperforms other neural network and decision tree variants on a wide range of non-performance-saturated tabular datasets and yields interpretable feature attributions plus insights into the global model behavior. Finally, for the first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly improving performance with unsupervised representation learning when unlabeled data is abundant.},
	journaltitle = {{arXiv}:1908.07442 [cs, stat]},
	author = {Arik, Sercan O. and Pfister, Tomas},
	urldate = {2021-10-12},
	date = {2020-12-09},
	eprinttype = {arxiv},
	eprint = {1908.07442},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/M7LWRHAZ/Arik and Pfister - 2020 - TabNet Attentive Interpretable Tabular Learning.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/WXBSGUC5/1908.html:text/html},
}


@article{popov_neural_2019,
	title = {Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data},
	url = {http://arxiv.org/abs/1909.06312},
	abstract = {Nowadays, deep neural networks ({DNNs}) have become the main instrument for machine learning tasks within a wide range of domains, including vision, {NLP}, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of {DNNs} over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees ({GBDT}), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles ({NODE}), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed {NODE} architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading {GBDT} packages on a large number of tabular datasets, we demonstrate the advantage of the proposed {NODE} architecture, which outperforms the competitors on most of the tasks. We open-source the {PyTorch} implementation of {NODE} and believe that it will become a universal framework for machine learning on tabular data.},
	journaltitle = {{arXiv}:1909.06312 [cs, stat]},
	author = {Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
	urldate = {2021-10-11},
	date = {2019-09-19},
	eprinttype = {arxiv},
	eprint = {1909.06312},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/B2P59N5X/Popov et al. - 2019 - Neural Oblivious Decision Ensembles for Deep Learn.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/WZCAJIP9/1909.html:text/html},
}


@article{abutbul_dnf-net_2020,
	title = {{DNF}-Net: A Neural Architecture for Tabular Data},
	url = {http://arxiv.org/abs/2006.06465},
	shorttitle = {{DNF}-Net},
	abstract = {A challenging open question in deep learning is how to handle tabular data. Unlike domains such as image and natural language processing, where deep architectures prevail, there is still no widely accepted neural architecture that dominates tabular data. As a step toward bridging this gap, we present {DNF}-Net a novel generic architecture whose inductive bias elicits models whose structure corresponds to logical Boolean formulas in disjunctive normal form ({DNF}) over affine soft-threshold decision terms. In addition, {DNF}-Net promotes localized decisions that are taken over small subsets of the features. We present an extensive empirical study showing that {DNF}-Nets significantly and consistently outperform {FCNs} over tabular data. With relatively few hyperparameters, {DNF}-Nets open the door to practical end-to-end handling of tabular data using neural networks. We present ablation studies, which justify the design choices of {DNF}-Net including the three inductive bias elements, namely, Boolean formulation, locality, and feature selection.},
	journaltitle = {{arXiv}:2006.06465 [cs, stat]},
	author = {Abutbul, Ami and Elidan, Gal and Katzir, Liran and El-Yaniv, Ran},
	urldate = {2021-10-12},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2006.06465},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/S2T6FV6Z/Abutbul et al. - 2020 - DNF-Net A Neural Architecture for Tabular Data.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/G3QLMAJ6/2006.html:text/html},
}


@article{shavitt_regularization_2018,
	title = {Regularization Learning Networks: Deep Learning for Tabular Datasets},
	url = {http://arxiv.org/abs/1805.06440},
	shorttitle = {Regularization Learning Networks},
	abstract = {Despite their impressive performance, Deep Neural Networks ({DNNs}) typically underperform Gradient Boosting Trees ({GBTs}) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of {DNNs} by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks ({RLNs}), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that {RLNs} significantly improve {DNNs} on tabular datasets, and achieve comparable results to {GBTs}, with the best performance achieved with an ensemble that combines {GBTs} and {RLNs}. {RLNs} produce extremely sparse networks, eliminating up to 99.8\% of the network edges and 82\% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. {RLNs} could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records. An open source implementation of {RLN} can be found at https://github.com/irashavitt/regularization\_learning\_networks.},
	journaltitle = {{arXiv}:1805.06440 [cs, stat]},
	author = {Shavitt, Ira and Segal, Eran},
	urldate = {2021-10-12},
	date = {2018-10-23},
	eprinttype = {arxiv},
	eprint = {1805.06440},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/WVN94N5U/Shavitt and Segal - 2018 - Regularization Learning Networks Deep Learning fo.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/EG9CBTRH/1805.html:text/html},
}


@article{kadra_regularization_2021,
	title = {Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data},
	url = {http://arxiv.org/abs/2106.11189},
	shorttitle = {Regularization is all you Need},
	abstract = {Tabular datasets are the last "unconquered castle" for deep learning, with traditional {ML} methods like Gradient-Boosted Decision Trees still performing strongly even against recent specialized neural architectures. In this paper, we hypothesize that the key to boosting the performance of neural networks lies in rethinking the joint and simultaneous application of a large set of modern regularization techniques. As a result, we propose regularizing plain Multilayer Perceptron ({MLP}) networks by searching for the optimal combination/cocktail of 13 regularization techniques for each dataset using a joint optimization over the decision on which regularizers to apply and their subsidiary hyperparameters. We empirically assess the impact of these regularization cocktails for {MLPs} on a large-scale empirical study comprising 40 tabular datasets and demonstrate that (i) well-regularized plain {MLPs} significantly outperform recent state-of-the-art specialized neural network architectures, and (ii) they even outperform strong traditional {ML} methods, such as {XGBoost}.},
	journaltitle = {{arXiv}:2106.11189 [cs]},
	author = {Kadra, Arlind and Lindauer, Marius and Hutter, Frank and Grabocka, Josif},
	urldate = {2021-10-11},
	date = {2021-06-21},
	eprinttype = {arxiv},
	eprint = {2106.11189},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/JGY28PF6/Kadra et al. - 2021 - Regularization is all you Need Simple Neural Nets.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/GMJ6GIJM/2106.html:text/html},
}