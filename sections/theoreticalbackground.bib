
@misc{klambauer_notes_2019,
    title={Lecture Notes: Deep Learning and Neural Networks I - Winter Semester 2019},
    author={Günter Klambauer and Sepp Hochreiter and P.-J. Hoedt and Ph. Renz},
    date = {2019-10},
    year={2019},
    note={Institute for Machine Learning, Johannes Kepler University Linz}    
}

@article{kotsiantis_decision_2013,
    title = {Decision trees: a recent overview},
    volume = {39},
    issn = {1573-7462},
    url = {https://doi.org/10.1007/s10462-011-9272-4},
    doi = {10.1007/s10462-011-9272-4},
    shorttitle = {Decision trees},
    abstract = {Decision tree techniques have been widely used to build classification models as such models closely resemble human reasoning and are easy to understand. This paper describes basic decision tree issues and current research points. Of course, a single article cannot be a complete review of all algorithms (also known induction classification trees), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
    pages = {261--283},
    number = {4},
    journaltitle = {Artificial Intelligence Review},
    shortjournal = {Artif Intell Rev},
    author = {Kotsiantis, S. B.},
    urldate = {2021-12-09},
    date = {2013-04-01},
    langid = {english},
    file = {Springer Full Text PDF:/home/ck/Zotero/storage/FDGEX6VG/Kotsiantis - 2013 - Decision trees a recent overview.pdf:application/pdf},
}

@article{kingma_auto-encoding_2014,
    title = {Auto-Encoding Variational Bayes},
    url = {http://arxiv.org/abs/1312.6114},
    abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
    journaltitle = {{arXiv}:1312.6114 [cs, stat]},
    author = {Kingma, Diederik P. and Welling, Max},
    urldate = {2021-12-09},
    date = {2014-05-01},
    eprinttype = {arxiv},
    eprint = {1312.6114},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goodfellow_generative_2014,
    title = {Generative Adversarial Networks},
    url = {http://arxiv.org/abs/1406.2661},
    abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
    journaltitle = {{arXiv}:1406.2661 [cs, stat]},
    author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    urldate = {2021-12-09},
    date = {2014-06-10},
    eprinttype = {arxiv},
    eprint = {1406.2661},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/65AFJJMK/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf},
}

@incollection{duelen_medicinal_2019,
    title = {Medicinal Biotechnology for Disease Modeling, Clinical Therapy, and Drug Discovery and Development},
    isbn = {978-3-030-22140-9},
    abstract = {Over the past decades, stem cell technology has revolutionalized medical biotechnology due to the unlimited self-renewal ability and differentiation capacity of stem cells to generate cells and tissues of the entire human body. Many efforts have focused on providing cutting-edge stem cell therapies in order to repair or replace damaged cells or tissues, hoping to ultimately cure devastating diseases. Undoubtedly, this novel technology guarantees a serial entrepreneur’s confidence in the future prospects of stem cell-based products and services. Here, we describe the state of the art of several applications of adult stem cells, as well as of embryonic and induced pluripotent stem cells in biotechnology that represent entrepreneurial opportunities. Although the contribution of stem cells to medical research is enormous, several hurdles still have to be overcome, including ethical and regulatory issues, functional maturation of stem cell progenitors, stringent manufacturing guidelines, immune rejection, and tumorigenicity.},
    pages = {89--128},
    author = {Duelen, Robin and Corvelyn, Marlies and Tortorella, Ilaria and Leonardi, Leonardo and Chai, Yoke and Sampaolesi, Maurilio},
    date = {2019-08-17},
    doi = {10.1007/978-3-030-22141-6_5},
    file = {Full Text PDF:/home/ck/Zotero/storage/UTVHY9P3/Duelen et al. - 2019 - Medicinal Biotechnology for Disease Modeling, Clin.pdf:application/pdf},
}

@article{durant_reoptimization_2002,
    title = {Reoptimization of {MDL} keys for use in drug discovery},
    volume = {42},
    issn = {0095-2338},
    doi = {10.1021/ci010132r},
    abstract = {For a number of years {MDL} products have exposed both 166 bit and 960 bit keysets based on 2D descriptors. These keysets were originally constructed and optimized for substructure searching. We report on improvements in the performance of {MDL} keysets which are reoptimized for use in molecular similarity. Classification performance for a test data set of 957 compounds was increased from 0.65 for the 166 bit keyset and 0.67 for the 960 bit keyset to 0.71 for a surprisal S/N pruned keyset containing 208 bits and 0.71 for a genetic algorithm optimized keyset containing 548 bits. We present an overview of the underlying technology supporting the definition of descriptors and the encoding of these descriptors into keysets. This technology allows definition of descriptors as combinations of atom properties, bond properties, and atomic neighborhoods at various topological separations as well as supporting a number of custom descriptors. These descriptors can then be used to set one or more bits in a keyset. We constructed various keysets and optimized their performance in clustering bioactive substances. Performance was measured using methodology developed by Briem and Lessel. "Directed pruning" was carried out by eliminating bits from the keysets on the basis of random selection, values of the surprisal of the bit, or values of the surprisal S/N ratio of the bit. The random pruning experiment highlighted the insensitivity of keyset performance for keyset lengths of more than 1000 bits. Contrary to initial expectations, pruning on the basis of the surprisal values of the various bits resulted in keysets which underperformed those resulting from random pruning. In contrast, pruning on the basis of the surprisal S/N ratio was found to yield keysets which performed better than those resulting from random pruning. We also explored the use of genetic algorithms in the selection of optimal keysets. Once more the performance was only a weak function of keyset size, and the optimizations failed to identify a single globally optimal keyset. Instead multiple, equally optimal keysets could be produced which had relatively low overlap of the descriptors they encoded.},
    pages = {1273--1280},
    number = {6},
    journaltitle = {Journal of Chemical Information and Computer Sciences},
    shortjournal = {J Chem Inf Comput Sci},
    author = {Durant, Joseph L. and Leland, Burton A. and Henry, Douglas R. and Nourse, James G.},
    date = {2002-12},
    pmid = {12444722},
    keywords = {Algorithms, Computational Biology, Databases, Factual, Drug Evaluation, Preclinical, Genetics, Pattern Recognition, Automated, Software, Structure-Activity Relationship},
    file = {Full Text:/home/ck/Zotero/storage/IMXRTCEH/Durant et al. - 2002 - Reoptimization of MDL keys for use in drug discove.pdf:application/pdf},
}

@online{noauthor_daylight_nodate,
    title = {Daylight Theory: {SMARTS} - A Language for Describing Molecular Patterns},
    url = {https://www.daylight.com/dayhtml/doc/theory/theory.smarts.html},
    urldate = {2021-12-08},
    file = {Daylight Theory\: SMARTS - A Language for Describing Molecular Patterns:/home/ck/Zotero/storage/U52C2UBZ/theory.smarts.html:text/html},
}

@online{sihag_beginners_2021,
    title = {A beginner’s guide for understanding Extended-Connectivity Fingerprints({ECFPs})},
    url = {https://chemicbook.com/2021/03/25/a-beginners-guide-for-understanding-extended-connectivity-fingerprints.html},
    abstract = {Extended-Connectivity Fingerprints({ECFPs}) are a type of molecular fingerprint explicitly designed to capture molecular features relevant to molecular activity. They are among the most popular similarity search tools in drug discovery and they are effectively used in a wide variety of applications.},
    titleaddon = {{ChemicBook}},
    author = {Sihag, Manish},
    urldate = {2021-12-08},
    date = {2021-03-25},
    langid = {english},
    file = {Snapshot:/home/ck/Zotero/storage/FJ29G8IP/a-beginners-guide-for-understanding-extended-connectivity-fingerprints.html:text/html},
}

@article{rogers_extended-connectivity_2010,
    title = {Extended-Connectivity Fingerprints},
    volume = {50},
    issn = {1549-9596},
    url = {https://doi.org/10.1021/ci100050t},
    doi = {10.1021/ci100050t},
    abstract = {Extended-connectivity fingerprints ({ECFPs}) are a novel class of topological fingerprints for molecular characterization. Historically, topological fingerprints were developed for substructure and similarity searching. {ECFPs} were developed specifically for structure−activity modeling. {ECFPs} are circular fingerprints with a number of useful qualities: they can be very rapidly calculated; they are not predefined and can represent an essentially infinite number of different molecular features (including stereochemical information); their features represent the presence of particular substructures, allowing easier interpretation of analysis results; and the {ECFP} algorithm can be tailored to generate different types of circular fingerprints, optimized for different uses. While the use of {ECFPs} has been widely adopted and validated, a description of their implementation has not previously been presented in the literature.},
    pages = {742--754},
    number = {5},
    journaltitle = {Journal of Chemical Information and Modeling},
    shortjournal = {J. Chem. Inf. Model.},
    author = {Rogers, David and Hahn, Mathew},
    urldate = {2021-12-08},
    date = {2010-05-24},
    note = {Publisher: American Chemical Society},
    file = {Full Text PDF:/home/ck/Zotero/storage/2RJPEVSV/Rogers and Hahn - 2010 - Extended-Connectivity Fingerprints.pdf:application/pdf;ACS Full Text Snapshot:/home/ck/Zotero/storage/CBKDW56J/ci100050t.html:text/html},
}

@article{carracedo-reboredo_review_2021,
    title = {A review on machine learning approaches and trends in drug discovery},
    volume = {19},
    issn = {2001-0370},
    url = {https://www.sciencedirect.com/science/article/pii/S2001037021003421},
    doi = {10.1016/j.csbj.2021.08.011},
    abstract = {Drug discovery aims at finding new compounds with specific chemical properties for the treatment of diseases. In the last years, the approach used in this search presents an important component in computer science with the skyrocketing of machine learning techniques due to its democratization. With the objectives set by the Precision Medicine initiative and the new challenges generated, it is necessary to establish robust, standard and reproducible computational methodologies to achieve the objectives set. Currently, predictive models based on Machine Learning have gained great importance in the step prior to preclinical studies. This stage manages to drastically reduce costs and research times in the discovery of new drugs. This review article focuses on how these new methodologies are being used in recent years of research. Analyzing the state of the art in this field will give us an idea of where cheminformatics will be developed in the short term, the limitations it presents and the positive results it has achieved. This review will focus mainly on the methods used to model the molecular data, as well as the biological problems addressed and the Machine Learning algorithms used for drug discovery in recent years.},
    pages = {4538--4558},
    journaltitle = {Computational and Structural Biotechnology Journal},
    shortjournal = {Computational and Structural Biotechnology Journal},
    author = {Carracedo-Reboredo, Paula and Liñares-Blanco, Jose and Rodríguez-Fernández, Nereida and Cedrón, Francisco and Novoa, Francisco J. and Carballal, Adrian and Maojo, Victor and Pazos, Alejandro and Fernandez-Lozano, Carlos},
    urldate = {2021-12-07},
    date = {2021-01-01},
    langid = {english},
    keywords = {Deep Learning, Machine Learning, Drug Discovery, Cheminformatics, Molecular Descriptors, {QSAR}},
    file = {Full Text:/home/ck/Zotero/storage/6GXXERNQ/Carracedo-Reboredo et al. - 2021 - A review on machine learning approaches and trends.pdf:application/pdf},
}

@online{noauthor_opensmiles_nodate,
    title = {{OpenSMILES} specification},
    url = {http://opensmiles.org/opensmiles.html},
    urldate = {2021-12-08},
}

@article{weininger_smiles_1988,
    title = {{SMILES}, a chemical language and information system. 1. Introduction to methodology and encoding rules},
    volume = {28},
    issn = {0095-2338},
    url = {https://pubs.acs.org/doi/abs/10.1021/ci00057a005},
    doi = {10.1021/ci00057a005},
    pages = {31--36},
    number = {1},
    journaltitle = {Journal of Chemical Information and Computer Sciences},
    shortjournal = {J. Chem. Inf. Comput. Sci.},
    author = {Weininger, David},
    urldate = {2021-12-08},
    date = {1988-02-01},
    note = {Publisher: American Chemical Society},
}

@article{ma_deep_2015,
    title = {Deep neural nets as a method for quantitative structure-activity relationships},
    volume = {55},
    issn = {1549-960X},
    doi = {10.1021/ci500747n},
    abstract = {Neural networks were widely used for quantitative structure-activity relationships ({QSAR}) in the 1990s. Because of various practical issues (e.g., slow on large problems, difficult to train, prone to overfitting, etc.), they were superseded by more robust methods like support vector machine ({SVM}) and random forest ({RF}), which arose in the early 2000s. The last 10 years has witnessed a revival of neural networks in the machine learning community thanks to new methods for preventing overfitting, more efficient training algorithms, and advancements in computer hardware. In particular, deep neural nets ({DNNs}), i.e. neural nets with more than one hidden layer, have found great successes in many applications, such as computer vision and natural language processing. Here we show that {DNNs} can routinely make better prospective predictions than {RF} on a set of large diverse {QSAR} data sets that are taken from Merck's drug discovery effort. The number of adjustable parameters needed for {DNNs} is fairly large, but our results show that it is not necessary to optimize them for individual data sets, and a single set of recommended parameters can achieve better performance than {RF} for most of the data sets we studied. The usefulness of the parameters is demonstrated on additional data sets not used in the calibration. Although training {DNNs} is still computationally intensive, using graphical processing units ({GPUs}) can make this issue manageable.},
    pages = {263--274},
    number = {2},
    journaltitle = {Journal of Chemical Information and Modeling},
    shortjournal = {J Chem Inf Model},
    author = {Ma, Junshui and Sheridan, Robert P. and Liaw, Andy and Dahl, George E. and Svetnik, Vladimir},
    date = {2015-02-23},
    pmid = {25635324},
    keywords = {Algorithms, Drug Discovery, Machine Learning, Neural Networks, Computer, Prospective Studies, Quantitative Structure-Activity Relationship, Support Vector Machine, Workflow},
    file = {Submitted Version:/home/ck/Zotero/storage/IRP63P8Q/Ma et al. - 2015 - Deep neural nets as a method for quantitative stru.pdf:application/pdf},
}


@article{cherkasov_qsar_2014,
    title = {{QSAR} Modeling: Where Have You Been? Where Are You Going To?},
    volume = {57},
    issn = {0022-2623},
    url = {https://doi.org/10.1021/jm4004285},
    doi = {10.1021/jm4004285},
    shorttitle = {{QSAR} Modeling},
    abstract = {Quantitative structure–activity relationship modeling is one of the major computational tools employed in medicinal chemistry. However, throughout its entire history it has drawn both praise and criticism concerning its reliability, limitations, successes, and failures. In this paper, we discuss (i) the development and evolution of {QSAR}; (ii) the current trends, unsolved problems, and pressing challenges; and (iii) several novel and emerging applications of {QSAR} modeling. Throughout this discussion, we provide guidelines for {QSAR} development, validation, and application, which are summarized in best practices for building rigorously validated and externally predictive {QSAR} models. We hope that this Perspective will help communications between computational and experimental chemists toward collaborative development and use of {QSAR} models. We also believe that the guidelines presented here will help journal editors and reviewers apply more stringent scientific standards to manuscripts reporting new {QSAR} studies, as well as encourage the use of high quality, validated {QSARs} for regulatory decision making.},
    pages = {4977--5010},
    number = {12},
    journaltitle = {Journal of Medicinal Chemistry},
    shortjournal = {J. Med. Chem.},
    author = {Cherkasov, Artem and Muratov, Eugene N. and Fourches, Denis and Varnek, Alexandre and Baskin, Igor I. and Cronin, Mark and Dearden, John and Gramatica, Paola and Martin, Yvonne C. and Todeschini, Roberto and Consonni, Viviana and Kuz’min, Victor E. and Cramer, Richard and Benigni, Romualdo and Yang, Chihae and Rathman, James and Terfloth, Lothar and Gasteiger, Johann and Richard, Ann and Tropsha, Alexander},
    urldate = {2021-12-07},
    date = {2014-06-26},
    note = {Publisher: American Chemical Society},
    file = {Full Text PDF:/home/ck/Zotero/storage/WWHB2WLM/Cherkasov et al. - 2014 - QSAR Modeling Where Have You Been Where Are You .pdf:application/pdf;ACS Full Text Snapshot:/home/ck/Zotero/storage/T6UBSQKB/jm4004285.html:text/html},
}

@inreference{noauthor_ic50_2021,
    title = {{IC}$_{\textrm{50}}$},
    rights = {Creative Commons Attribution-{ShareAlike} License},
    url = {https://en.wikipedia.org/w/index.php?title=IC50&oldid=1034669993},
    booktitle = {Wikipedia},
    urldate = {2021-12-08},
    date = {2021-07-21},
    langid = {english},
    note = {Page Version {ID}: 1034669993},
    file = {Snapshot:/home/ck/Zotero/storage/N7BJVBCK/index.html:text/html},
}

@article{hansch_correlation_1962,
    title = {Correlation of Biological Activity of Phenoxyacetic Acids with Hammett Substituent Constants and Partition Coefficients},
    volume = {194},
    rights = {1962 Nature Publishing Group},
    issn = {1476-4687},
    url = {https://www.nature.com/articles/194178b0},
    doi = {10.1038/194178b0},
    abstract = {{IN} the ‘two-point attachment’ theory1 on the mechanism of action for growth regulators of the auxin type we have assumed as a working hypothesis that the reaction between auxin and substrate is more chemical than physical in nature and that covalent-bond formation is a possibility. The need for an aromatic ring with at least one unsubstituted position for auxin activity2 confirms our original belief that an ortho position in the phenoxyacetic acids is most suitable stereoelectronically to serve as one of the two points which attach the molecule to the plant substrate. The carboxyl serves as the second point, permitting the auxin to form a ring when joined to a plant protein. This reaction mechanism is supported by the work of Bonner et al.3,4.},
    pages = {178--180},
    number = {4824},
    journaltitle = {Nature},
    author = {Hansch, Corwin and Maloney, Peyton P. and Fujita, Toshio and Muir, Robert M.},
    urldate = {2021-12-08},
    date = {1962-04},
    langid = {english},
    note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 4824
Primary\_atype: Research
Publisher: Nature Publishing Group},
    keywords = {Humanities and Social Sciences, multidisciplinary, Science},
    file = {Snapshot:/home/ck/Zotero/storage/Z9HKP93E/194178b0.html:text/html},
}

@article{kim_artificial_2020,
    title = {Artificial Intelligence in Drug Discovery: A Comprehensive Review of Data-driven and Machine Learning Approaches},
    volume = {25},
    issn = {1976-3816},
    url = {https://doi.org/10.1007/s12257-020-0049-y},
    doi = {10.1007/s12257-020-0049-y},
    shorttitle = {Artificial Intelligence in Drug Discovery},
    abstract = {As expenditure on drug development increases exponentially, the overall drug discovery process requires a sustainable revolution. Since artificial intelligence ({AI}) is leading the fourth industrial revolution, {AI} can be considered as a viable solution for unstable drug research and development. Generally, {AI} is applied to fields with sufficient data such as computer vision and natural language processing, but there are many efforts to revolutionize the existing drug discovery process by applying {AI}. This review provides a comprehensive, organized summary of the recent research trends in {AI}-guided drug discovery process including target identification, hit identification, {ADMET} prediction, lead optimization, and drug repositioning. The main data sources in each field are also summarized in this review. In addition, an in-depth analysis of the remaining challenges and limitations will be provided, and proposals for promising future directions in each of the aforementioned areas.},
    pages = {895--930},
    number = {6},
    journaltitle = {Biotechnology and Bioprocess Engineering},
    shortjournal = {Biotechnol Bioproc E},
    author = {Kim, Hyunho and Kim, Eunyoung and Lee, Ingoo and Bae, Bongsung and Park, Minsu and Nam, Hojung},
    urldate = {2021-12-07},
    date = {2020-12-01},
    langid = {english},
    file = {Springer Full Text PDF:/home/ck/Zotero/storage/7P4SBSJ7/Kim et al. - 2020 - Artificial Intelligence in Drug Discovery A Compr.pdf:application/pdf},
}

@article{castro_polynomial_2009,
    title = {Polynomial calculation of the Shapley value based on sampling},
    volume = {36},
    pages = {1726--1730},
    number = {5},
    journaltitle = {Computers \& Operations Research},
    author = {Castro, Javier and Gómez, Daniel and Tejada, Juan},
    date = {2009},
    note = {Publisher: Elsevier},
    file = {Full Text:/home/ck/Zotero/storage/N6T6UU4X/Castro et al. - 2009 - Polynomial calculation of the Shapley value based .pdf:application/pdf},
}

@book{shapley1951notes,
  title={Notes on the N-person Game--I: Characteristic-point Solutions of the Four-person Game},
  author={Shapley, Lloyd S},
  year={1951},
  publisher={Rand Corporation}
}

@inproceedings{shrikumar_learning_2017,
    title = {Learning Important Features Through Propagating Activation Differences},
    url = {https://proceedings.mlr.press/v70/shrikumar17a.html},
    abstract = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present {DeepLIFT} (Deep Learning Important {FeaTures}), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. {DeepLIFT} compares the activation of each neuron to its `reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, {DeepLIFT} can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply {DeepLIFT} to models trained on {MNIST} and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/{qKb}7pL code: http://goo.gl/{RM}8jvH},
    eventtitle = {International Conference on Machine Learning},
    pages = {3145--3153},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
    urldate = {2021-11-14},
    date = {2017-07-17},
    langid = {english},
    note = {{ISSN}: 2640-3498},
    file = {Full Text PDF:/home/ck/Zotero/storage/PPWX9JRP/Shrikumar et al. - 2017 - Learning Important Features Through Propagating Ac.pdf:application/pdf;Supplementary PDF:/home/ck/Zotero/storage/ENH8CK4V/Shrikumar et al. - 2017 - Learning Important Features Through Propagating Ac.pdf:application/pdf},
}

@article{simonyan_deep_2014,
    title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
    url = {http://arxiv.org/abs/1312.6034},
    shorttitle = {Deep Inside Convolutional Networks},
    abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks ({ConvNets}). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a {ConvNet}. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification {ConvNets}. Finally, we establish the connection between the gradient-based {ConvNet} visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
    journaltitle = {{arXiv}:1312.6034 [cs]},
    author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
    urldate = {2021-11-14},
    date = {2014-04-19},
    eprinttype = {arxiv},
    eprint = {1312.6034},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/AZ7GG49A/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/J96L7GQ8/1312.html:text/html},
}

@article{sundararajan_axiomatic_2017,
    title = {Axiomatic Attribution for Deep Networks},
    url = {http://arxiv.org/abs/1703.01365},
    abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
    journaltitle = {{arXiv}:1703.01365 [cs]},
    author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
    urldate = {2021-11-14},
    date = {2017-06-12},
    eprinttype = {arxiv},
    eprint = {1703.01365},
    keywords = {Computer Science - Machine Learning},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/YPT6SICM/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/EVULQMEP/1703.html:text/html},
}

@article{lipton_mythos_2017,
    title = {The Mythos of Model Interpretability},
    url = {http://arxiv.org/abs/1606.03490},
    abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
    journaltitle = {{arXiv}:1606.03490 [cs, stat]},
    author = {Lipton, Zachary C.},
    urldate = {2021-11-13},
    date = {2017-03-06},
    eprinttype = {arxiv},
    eprint = {1606.03490},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/WTF8NJ6L/Lipton - 2017 - The Mythos of Model Interpretability.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/RZ6L5IQN/1606.html:text/html},
}

@online{darpa_xai,
    title = {Explainable Artificial Intelligence},
    url = {https://www.darpa.mil/program/explainable-artificial-intelligence},
    urldate = {2021-11-13},
    file = {Explainable Artificial Intelligence:/home/ck/Zotero/storage/ASLAXS8C/explainable-artificial-intelligence.html:text/html},
}

@article{marcinkevics_interpretability_2020,
    title = {Interpretability and Explainability: A Machine Learning Zoo Mini-tour},
    url = {http://arxiv.org/abs/2012.01805},
    shorttitle = {Interpretability and Explainability},
    abstract = {In this review, we examine the problem of designing interpretable and explainable machine learning models. Interpretability and explainability lie at the core of many machine learning and statistical applications in medicine, economics, law, and natural sciences. Although interpretability and explainability have escaped a clear universal definition, many techniques motivated by these properties have been developed over the recent 30 years with the focus currently shifting towards deep learning methods. In this review, we emphasise the divide between interpretability and explainability and illustrate these two different research directions with concrete examples of the state-of-the-art. The review is intended for a general machine learning audience with interest in exploring the problems of interpretation and explanation beyond logistic regression or random forest variable importance. This work is not an exhaustive literature survey, but rather a primer focusing selectively on certain lines of research which the authors found interesting or informative.},
    journaltitle = {{arXiv}:2012.01805 [cs]},
    author = {Marcinkevičs, Ričards and Vogt, Julia E.},
    urldate = {2021-10-11},
    date = {2020-12-03},
    eprinttype = {arxiv},
    eprint = {2012.01805},
    keywords = {Computer Science - Machine Learning},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/4CXEUZNC/Marcinkevičs and Vogt - 2020 - Interpretability and Explainability A Machine Lea.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/WG4A3UTQ/2012.html:text/html},
}

@article{barredo_arrieta_explainable_2020,
    title = {Explainable Artificial Intelligence ({XAI}): Concepts, taxonomies, opportunities and challenges toward responsible {AI}},
    volume = {58},
    issn = {1566-2535},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
    doi = {10.1016/j.inffus.2019.12.012},
    shorttitle = {Explainable Artificial Intelligence ({XAI})},
    abstract = {In the last few years, Artificial Intelligence ({AI}) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of {AI} (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called {eXplainable} {AI} ({XAI}) field, which is widely acknowledged as a crucial feature for the practical deployment of {AI} models. The overview presented in this article examines the existing literature and contributions already done in the field of {XAI}, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by {XAI}, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of {AI} methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of {XAI} with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of {AI} in their activity sectors, without any prior bias for its lack of interpretability.},
    pages = {82--115},
    journaltitle = {Information Fusion},
    shortjournal = {Information Fusion},
    author = {Barredo Arrieta, Alejandro and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
    urldate = {2021-11-13},
    date = {2020-06-01},
    langid = {english},
    keywords = {Accountability, Comprehensibility, Data Fusion, Deep Learning, Explainable Artificial Intelligence, Fairness, Interpretability, Machine Learning, Privacy, Responsible Artificial Intelligence, Transparency},
    file = {Accepted Version:/home/ck/Zotero/storage/8ZWI3DZR/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@article{palacio_xai_2021,
    title = {{XAI} Handbook: Towards a Unified Framework for Explainable {AI}},
    url = {http://arxiv.org/abs/2105.06677},
    shorttitle = {{XAI} Handbook},
    abstract = {The field of explainable {AI} ({XAI}) has quickly become a thriving and prolific community. However, a silent, recurrent and acknowledged issue in this area is the lack of consensus regarding its terminology. In particular, each new contribution seems to rely on its own (and often intuitive) version of terms like "explanation" and "interpretation". Such disarray encumbers the consolidation of advances in the field towards the fulfillment of scientific and regulatory demands e.g., when comparing methods or establishing their compliance with respect to biases and fairness constraints. We propose a theoretical framework that not only provides concrete definitions for these terms, but it also outlines all steps necessary to produce explanations and interpretations. The framework also allows for existing contributions to be re-contextualized such that their scope can be measured, thus making them comparable to other methods. We show that this framework is compliant with desiderata on explanations, on interpretability and on evaluation metrics. We present a use-case showing how the framework can be used to compare {LIME}, {SHAP} and {MDNet}, establishing their advantages and shortcomings. Finally, we discuss relevant trends in {XAI} as well as recommendations for future work, all from the standpoint of our framework.},
    journaltitle = {{arXiv}:2105.06677 [cs]},
    author = {Palacio, Sebastian and Lucieri, Adriano and Munir, Mohsin and Hees, Jörn and Ahmed, Sheraz and Dengel, Andreas},
    urldate = {2021-11-12},
    date = {2021-05-14},
    eprinttype = {arxiv},
    eprint = {2105.06677},
    keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/CS3UKAVR/Palacio et al. - 2021 - XAI Handbook Towards a Unified Framework for Expl.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/AGSA4529/2105.html:text/html},
}

@article{linardatos_explainable_2021,
    title = {Explainable {AI}: A Review of Machine Learning Interpretability Methods},
    volume = {23},
    rights = {http://creativecommons.org/licenses/by/3.0/},
    url = {https://www.mdpi.com/1099-4300/23/1/18},
    doi = {10.3390/e23010018},
    shorttitle = {Explainable {AI}},
    abstract = {Recent advances in artificial intelligence ({AI}) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into \&ldquo;black box\&rdquo; approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence ({XAI}), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
    pages = {18},
    number = {1},
    journaltitle = {Entropy},
    author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
    urldate = {2021-11-10},
    date = {2021-01},
    langid = {english},
    note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
    keywords = {black-box, explainability, fairness, interpretability, machine learning, sensitivity, xai},
    file = {Full Text PDF:/home/ck/Zotero/storage/NP388Z4V/Linardatos et al. - 2021 - Explainable AI A Review of Machine Learning Inter.pdf:application/pdf;Snapshot:/home/ck/Zotero/storage/ZF7NVUWT/18.html:text/html},
}

@article{peters_sparse_2019,
    title = {Sparse Sequence-to-Sequence Models},
    url = {http://arxiv.org/abs/1905.05702},
    abstract = {Sequence-to-sequence models are a powerful workhorse of {NLP}. Most variants employ a softmax transformation in both their attention mechanism and output layer, leading to dense alignments and strictly positive output probabilities. This density is wasteful, making models less interpretable and assigning probability mass to many implausible outputs. In this paper, we propose sparse sequence-to-sequence models, rooted in a new family of \${\textbackslash}alpha\$-entmax transformations, which includes softmax and sparsemax as particular cases, and is sparse for any \${\textbackslash}alpha {\textgreater} 1\$. We provide fast algorithms to evaluate these transformations and their gradients, which scale well for large vocabulary sizes. Our models are able to produce sparse alignments and to assign nonzero probability to a short list of plausible outputs, sometimes rendering beam search exact. Experiments on morphological inflection and machine translation reveal consistent gains over dense models.},
    journaltitle = {{arXiv}:1905.05702 [cs]},
    author = {Peters, Ben and Niculae, Vlad and Martins, André F. T.},
    urldate = {2021-11-01},
    date = {2019-06-12},
    eprinttype = {arxiv},
    eprint = {1905.05702},
    keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/L8Y9D8QK/Peters et al. - 2019 - Sparse Sequence-to-Sequence Models.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/TYHWGGXF/1905.html:text/html},
}

@inproceedings{martins_softmax_2016,
    title = {From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification},
    url = {https://proceedings.mlr.press/v48/martins16.html},
    shorttitle = {From Softmax to Sparsemax},
    abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
    eventtitle = {International Conference on Machine Learning},
    pages = {1614--1623},
    booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Martins, Andre and Astudillo, Ramon},
    urldate = {2021-11-01},
    date = {2016-06-11},
    langid = {english},
    note = {{ISSN}: 1938-7228},
    file = {Full Text PDF:/home/ck/Zotero/storage/CCCNTATQ/Martins and Astudillo - 2016 - From Softmax to Sparsemax A Sparse Model of Atten.pdf:application/pdf},
}

@article{hoffer_train_2017,
    title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
    url = {https://arxiv.org/abs/1705.08741v2},
    shorttitle = {Train longer, generalize better},
    abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on {MNIST}, {CIFAR}-10, {CIFAR}-100 and {ImageNet}. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
    author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
    urldate = {2021-10-31},
    date = {2017-05-24},
    langid = {english},
    file = {Snapshot:/home/ck/Zotero/storage/ECBB2ZVQ/1705.html:text/html;Full Text PDF:/home/ck/Zotero/storage/7ZGUEDFD/Hoffer et al. - 2017 - Train longer, generalize better closing the gener.pdf:application/pdf},
}

@article{hochreiter_long_1997,
    title = {Long Short-Term Memory},
    volume = {9},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory ({LSTM}). Truncating the gradient where this does not do harm, {LSTM} can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. {LSTM} is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, {LSTM} leads to many more successful runs, and learns much faster. {LSTM} also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    pages = {1735--1780},
    number = {8},
    journaltitle = {Neural Computation},
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    date = {1997-11},
    note = {Conference Name: Neural Computation},
}

@inproceedings{dauphin_language_2017,
    title = {Language Modeling with Gated Convolutional Networks},
    url = {https://proceedings.mlr.press/v70/dauphin17a.html},
    abstract = {The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al. (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the {WikiText}-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.},
    eventtitle = {International Conference on Machine Learning},
    pages = {933--941},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Dauphin, Yann N. and Fan, Angela and Auli, Michael and Grangier, David},
    urldate = {2021-10-31},
    date = {2017-07-17},
    langid = {english},
    note = {{ISSN}: 2640-3498},
    file = {Full Text PDF:/home/ck/Zotero/storage/Z64SYIUY/Dauphin et al. - 2017 - Language Modeling with Gated Convolutional Network.pdf:application/pdf},
}

@inproceedings{gehring_convolutional_2017,
    title = {Convolutional Sequence to Sequence Learning},
    url = {https://proceedings.mlr.press/v70/gehring17a.html},
    abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the {GPU} hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep {LSTM} setup of Wu et al. (2016) on both {WMT}’14 English-German and {WMT}’14 English-French translation at an order of magnitude faster speed, both on {GPU} and {CPU}.},
    eventtitle = {International Conference on Machine Learning},
    pages = {1243--1252},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning},
    publisher = {{PMLR}},
    author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
    urldate = {2021-10-31},
    date = {2017-07-17},
    langid = {english},
    note = {{ISSN}: 2640-3498},
    file = {Full Text PDF:/home/ck/Zotero/storage/RRJ82A4X/Gehring et al. - 2017 - Convolutional Sequence to Sequence Learning.pdf:application/pdf;Supplementary PDF:/home/ck/Zotero/storage/SFSCQV3D/Gehring et al. - 2017 - Convolutional Sequence to Sequence Learning.pdf:application/pdf},
}

@article{ioffe_batch_2015,
    title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    url = {https://arxiv.org/abs/1502.03167v3},
    shorttitle = {Batch Normalization},
    abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on {ImageNet} classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
    author = {Ioffe, Sergey and Szegedy, Christian},
    urldate = {2021-10-31},
    date = {2015-02-11},
    langid = {english},
    file = {Full Text PDF:/home/ck/Zotero/storage/89HWGMK2/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf:application/pdf},
}


@book{minsky_perceptrons_1969,
    location = {Oxford, England},
    title = {Perceptrons},
    series = {Perceptrons},
    abstract = {Theories of automata, artificial intelligence, and pattern recognition, blended into an original contribution to several sciences, including psychology.  Harvard Book List (edited) 1971 \#303 ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
    publisher = {M.I.T. Press},
    author = {Minsky, Marvin and Papert, Seymour},
    date = {1969},
    file = {Snapshot:/home/ck/Zotero/storage/85Q2LE6X/1969-35017-000.html:text/html},
}

@article{lecun_deep_2015,
    title = {Deep learning},
    volume = {521},
    rights = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
    issn = {1476-4687},
    url = {https://www.nature.com/articles/nature14539},
    doi = {10.1038/nature14539},
    abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
    pages = {436--444},
    number = {7553},
    journaltitle = {Nature},
    author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
    urldate = {2021-10-24},
    date = {2015-05},
    langid = {english},
    note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7553
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Computer science;Mathematics and computing
Subject\_term\_id: computer-science;mathematics-and-computing},
    keywords = {Computer science, Mathematics and computing},
    file = {Snapshot:/home/ck/Zotero/storage/UDIQ34L4/nature14539.html:text/html},
}

@article{schmidhuber_deep_2015,
    title = {Deep Learning in Neural Networks: An Overview},
    volume = {61},
    issn = {08936080},
    url = {http://arxiv.org/abs/1404.7828},
    doi = {10.1016/j.neunet.2014.09.003},
    shorttitle = {Deep Learning in Neural Networks},
    abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
    pages = {85--117},
    journaltitle = {Neural Networks},
    shortjournal = {Neural Networks},
    author = {Schmidhuber, Juergen},
    urldate = {2021-10-24},
    date = {2015-01},
    eprinttype = {arxiv},
    eprint = {1404.7828},
    keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/TKEZ37ML/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/GGGJA6JN/1404.html:text/html},
}

@article{prokhorenkova_catboost_2019,
    title = {{CatBoost}: unbiased boosting with categorical features},
    url = {http://arxiv.org/abs/1706.09516},
    shorttitle = {{CatBoost}},
    abstract = {This paper presents the key algorithmic techniques behind {CatBoost}, a new gradient boosting toolkit. Their combination leads to {CatBoost} outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in {CatBoost} are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
    journaltitle = {{arXiv}:1706.09516 [cs]},
    author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
    urldate = {2021-10-23},
    date = {2019-01-20},
    eprinttype = {arxiv},
    eprint = {1706.09516},
    keywords = {Computer Science - Machine Learning},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/HI4P4CN6/Prokhorenkova et al. - 2019 - CatBoost unbiased boosting with categorical featu.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/5U4SE65Y/1706.html:text/html},
}

@inproceedings{ke_lightgbm_2017,
    title = {{LightGBM}: A Highly Efficient Gradient Boosting Decision Tree},
    volume = {30},
    url = {https://papers.nips.cc/paper/2017/hash/6449f44 a102fde848669bdd9eb6b76fa-Abstract.html},
    shorttitle = {{LightGBM}},
    booktitle = {Advances in Neural Information Processing Systems},
    publisher = {Curran Associates, Inc.},
    author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
    urldate = {2021-10-23},
    date = {2017},
    file = {Full Text PDF:/home/ck/Zotero/storage/2REPQEZE/Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Dec.pdf:application/pdf},
}



@article{friedman_greedy_2001,
    title = {Greedy function approximation: A gradient boosting machine.},
    volume = {29},
    issn = {0090-5364, 2168-8966},
    url = {https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full},
    doi = {10.1214/aos/1013203451},
    shorttitle = {Greedy function approximation},
    abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “{TreeBoost}” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
    pages = {1189--1232},
    number = {5},
    journaltitle = {The Annals of Statistics},
    author = {Friedman, Jerome H.},
    urldate = {2021-10-21},
    date = {2001-10},
    note = {Publisher: Institute of Mathematical Statistics},
    keywords = {boosting, 62-02, 62-07, 62-08, 62G08, 62H30, 68T10, decision trees, Function estimation, robust nonparametric regression},
    file = {Snapshot:/home/ck/Zotero/storage/VWYPZYE4/1013203451.html:text/html;Full Text PDF:/home/ck/Zotero/storage/F9EL4S3Z/Friedman - 2001 - Greedy function approximation A gradient boosting.pdf:application/pdf},
}
@inreference{noauthor_gradient_2021,
    title = {Gradient boosting},
    rights = {Creative Commons Attribution-{ShareAlike} License},
    url = {https://en.wikipedia.org/w/index.php?title=Gradient_boosting&oldid=1050325665},
    abstract = {Gradient boosting is a machine learning technique for regression, classification and other tasks, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient boosted trees, which usually outperforms random forest. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.},
    booktitle = {Wikipedia},
    urldate = {2021-10-19},
    date = {2021-10-17},
    langid = {english},
    note = {Page Version {ID}: 1050325665},
}

@article{breiman_random_2001,
    title = {Random Forests},
    volume = {45},
    issn = {1573-0565},
    url = {https://doi.org/10.1023/A:1010933404324},
    doi = {10.1023/A:1010933404324},
    abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
    pages = {5--32},
    number = {1},
    journaltitle = {Machine Learning},
    shortjournal = {Machine Learning},
    author = {Breiman, Leo},
    urldate = {2021-10-17},
    date = {2001-10-01},
    langid = {english},
    file = {Springer Full Text PDF:/home/ck/Zotero/storage/N56FB4JA/Breiman - 2001 - Random Forests.pdf:application/pdf},
}

@article{ho_random_1998,
    title = {The random subspace method for constructing decision forests},
    volume = {20},
    issn = {1939-3539},
    doi = {10.1109/34.709601},
    abstract = {Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy.},
    pages = {832--844},
    number = {8},
    journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
    author = {Ho, Tin Kam},
    date = {1998-08},
    note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
    keywords = {Binary trees, Classification tree analysis, Clustering algorithms, Decision trees, Stochastic systems, Support vector machine classification, Support vector machines, Tin, Training data},
    file = {IEEE Xplore Abstract Record:/home/ck/Zotero/storage/BAZKCUY5/709601.html:text/html},
}

@online{noauthor_110_nodate,
    title = {1.10. Decision Trees},
    url = {https://scikit-learn/stable/modules/tree.html},
    abstract = {Decision Trees ({DTs}) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning s...},
    titleaddon = {scikit-learn},
    urldate = {2021-10-19},
    langid = {english},
}

@article{quinlan_induction_1986,
    title = {Induction of decision trees},
    volume = {1},
    issn = {1573-0565},
    url = {https://doi.org/10.1007/BF00116251},
    doi = {10.1007/BF00116251},
    abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, {ID}3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
    pages = {81--106},
    number = {1},
    journaltitle = {Machine Learning},
    shortjournal = {Mach Learn},
    author = {Quinlan, J. R.},
    urldate = {2021-10-19},
    date = {1986-03-01},
    langid = {english},
    file = {Springer Full Text PDF:/home/ck/Zotero/storage/WD7NSRFS/Quinlan - 1986 - Induction of decision trees.pdf:application/pdf},
}

@article{sagi_ensemble_2018,
    title = {Ensemble learning: A survey},
    volume = {8},
    issn = {1942-4795},
    url = {https://onlinelibrary.wiley.com /doi/abs/10.1002/widm.1249},
    doi = {10.1002/widm.1249},
    shorttitle = {Ensemble learning},
    abstract = {Ensemble methods are considered the state-of-the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state-of-the-art ensemble methods and discusses current challenges and trends in the field. This article is categorized under: Algorithmic Development {\textgreater} Ensemble Methods Technologies {\textgreater} Machine Learning Technologies {\textgreater} Classification},
    pages = {e1249},
    number = {4},
    journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
    author = {Sagi, Omer and Rokach, Lior},
    urldate = {2021-10-17},
    date = {2018},
    langid = {english},
    % note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1249},
    keywords = {boosting, classifier combination, ensemble models, machine-learning, mixtures of experts, multiple classifier system, random forest},
    file = {Full Text PDF:/home/ck/Zotero/storage/3YVFI3YI/Sagi and Rokach - 2018 - Ensemble learning A survey.pdf:application/pdf;Snapshot:/home/ck/Zotero/storage/QVGLCAZ9/widm.html:text/html},
}

@article{zhuang_comprehensive_2020,
    title = {A Comprehensive Survey on Transfer Learning},
    url = {http://arxiv.org/abs/1911.02685},
    abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
    % journaltitle = {{arXiv}:1911.02685 [cs, stat]},
    author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
    urldate = {2021-10-16},
    date = {2020-06-23},
    eprinttype = {arxiv},
    eprint = {1911.02685},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/ZHSVJ3ME/Zhuang et al. - 2020 - A Comprehensive Survey on Transfer Learning.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/IHYTHCX2/1911.html:text/html},
}


@report{settles_active_2009,
    title = {Active Learning Literature Survey},
    url = {https://minds.wisconsin.edu/handle/1793/60660},
    abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difficult, time-consuming, or expensive to obtain.

This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.},
    institution = {University of Wisconsin-Madison Department of Computer Sciences},
    type = {Technical Report},
    author = {Settles, Burr},
    urldate = {2021-10-16},
    date = {2009},
    langid = {english},
    note = {Accepted: 2012-03-15T17:23:56Z},
    file = {Full Text PDF:/home/ck/Zotero/storage/HXJ3RS94/Settles - 2009 - Active Learning Literature Survey.pdf:application/pdf;Snapshot:/home/ck/Zotero/storage/23B7IC4U/60660.html:text/html},
}

@article{kolesnikov_revisiting_2019,
    title = {Revisiting Self-Supervised Visual Representation Learning},
    url = {http://arxiv.org/abs/1901.09005},
    abstract = {Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks ({CNN}), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for {CNN} design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.},
    journaltitle = {{arXiv}:1901.09005 [cs]},
    author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
    urldate = {2021-10-16},
    date = {2019-01-25},
    eprinttype = {arxiv},
    eprint = {1901.09005},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/U9WJW7AH/Kolesnikov et al. - 2019 - Revisiting Self-Supervised Visual Representation L.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/ERHEV5BL/1901.html:text/html},
}

@article{ligthart_analyzing_2021,
    title = {Analyzing the effectiveness of semi-supervised learning approaches for opinion spam classification},
    volume = {101},
    issn = {1568-4946},
    url = {https://www.sciencedirect.com/science/article/pii/S1568494620309625},
    doi = {10.1016/j.asoc.2020.107023},
    abstract = {Opinion spam detection is concerned with identifying fake reviews that are deliberately placed to either promote or discredit a product. Opinionated social media like product reviews are increasingly important resources for people as well as businesses in the decision-making process and can be easily manipulated by opportunistic individuals. To reduce this increasing impact of opinion spams, opinion spam detection approaches have been proposed, which adopt mostly supervised classification methods. However, in practice, the provided data is largely not labeled and therefore semi-supervised learning approaches are required instead. To this end, this study aims to analyze the effectiveness of several semi-supervised learning approaches for opinion spam classification. Four different semi-supervised methods are evaluated on a dataset of both genuine and deceptive hotel reviews. The results are compared with several traditional classification methods using the same amount of labeled data. According to this study, the self-training algorithm with Naive Bayes as the base classifier yields 93\% accuracy. Results show that self-training is the only approach, out of the four tested semi-supervised models, that outperforms traditional supervised classification models when limited data is available. This study further shows that self-training can mitigate labeling efforts while retaining high model performance, which is useful for scenarios where limited data is available or retrieving labeled data is more costly.},
    pages = {107023},
    journaltitle = {Applied Soft Computing},
    shortjournal = {Applied Soft Computing},
    author = {Ligthart, Alexander and Catal, Cagatay and Tekinerdogan, Bedir},
    urldate = {2021-10-16},
    date = {2021-03-01},
    langid = {english},
    keywords = {Fake reviews, Machine learning, Opinion spam detection, Semi-supervised learning},
    file = {ScienceDirect Full Text PDF:/home/ck/Zotero/storage/IRG7NE3D/Ligthart et al. - 2021 - Analyzing the effectiveness of semi-supervised lea.pdf:application/pdf},
}

@collection{chapelle_semi-supervised_2006,
    location = {Cambridge, Mass},
    title = {Semi-supervised learning},
    isbn = {978-0-262-03358-9},
    series = {Adaptive computation and machine learning},
    pagetotal = {508},
    publisher = {{MIT} Press},
    editor = {Chapelle, Olivier and Schölkopf, Bernhard and Zien, Alexander},
    date = {2006},
    langid = {english},
    note = {{OCLC}: ocm64898359},
    keywords = {Supervised learning (Machine learning)},
    file = {Chapelle et al. - 2006 - Semi-supervised learning.pdf:/home/ck/Zotero/storage/DCQFIKDJ/Chapelle et al. - 2006 - Semi-supervised learning.pdf:application/pdf},
}

@book{sutton_reinforcement_nodate,
    title = {Reinforcement learning: An introduction},
    shorttitle = {Reinforcement learning},
    publisher = {{MIT} press},
    author = {Sutton, Richard S. and Barto, Andrew G.},
    date = {2018},
    file = {Full Text:/home/ck/Zotero/storage/LSJ375T9/Sutton and Barto - 2018 - Reinforcement learning An introduction.pdf:application/pdf;Snapshot:/home/ck/Zotero/storage/CLAD3W6P/books.html:text/html},
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{bishop_pattern_2006,
    location = {New York},
    title = {Pattern recognition and machine learning},
    isbn = {978-0-387-31073-2},
    series = {Information science and statistics},
    pagetotal = {738},
    publisher = {Springer},
    author = {Bishop, Christopher M.},
    date = {2006},
    langid = {english},
    keywords = {Machine learning, Pattern perception},
    file = {Bishop - 2006 - Pattern recognition and machine learning.pdf:/home/ck/Zotero/storage/5ZKRFLKM/Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf},
}

@online{brownlee_14_2019,
    title = {14 Different Types of Learning in Machine Learning},
    url = {https://machinelearningmastery.com/types-of-learning-in-machine-learning/},
    abstract = {Machine learning is a large field of study that overlaps with and inherits ideas from many related fields such as […]},
    titleaddon = {Machine Learning Mastery},
    author = {Brownlee, Jason},
    urldate = {2021-10-15},
    date = {2019-11-10},
    langid = {american},
    file = {Snapshot:/home/ck/Zotero/storage/HAS3JJNZ/types-of-learning-in-machine-learning.html:text/html},
}

@book{mitchell_machine_1997,
    location = {New York},
    title = {Machine Learning},
    isbn = {978-0-07-042807-2},
    abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the {McGraw}-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning—including probability and statistics, artificial intelligence, and neural networks—unifying them all in a logical and coherent manner.},
    publisher = {{McGraw}-Hill},
    author = {Mitchell, Tom M.},
    date = {1997},
    keywords = {01624 105 book shelf ai learn algorithm},
}

@book{kurzweil_singularity_2006,
    location = {New York},
    title = {The Singularity Is Near: When Humans Transcend Biology},
    isbn = {978-0-7394-6626-1},
    shorttitle = {The Singularity Is Near},
    abstract = {“Startling in scope and bravado.” —Janet Maslin, The New York Times“Artfully envisions a breathtakingly better world.” —Los Angeles Times“Elaborate, smart and persuasive.” —The Boston Globe“A pleasure to read.” —The Wall Street {JournalOne} of {CBS} News’s Best Fall Books of 2005 • Among St Louis Post-Dispatch’s Best Nonfiction Books of 2005 • One of Amazon.com’s Best Science Books of 2005A radical and optimistic view of the future course of human development from the bestselling author of How to Create a Mind and The Singularity is Nearer who Bill Gates calls “the best person I know at predicting the future of artificial intelligence”For over three decades, Ray Kurzweil has been one of the most respected and provocative advocates of the role of technology in our future. In his classic The Age of Spiritual Machines, he argued that computers would soon rival the full range of human intelligence at its best. Now he examines the next step in this inexorable evolutionary process: the union of human and machine, in which the knowledge and skills embedded in our brains will be combined with the vastly greater capacity, speed, and knowledge-sharing ability of our creations.},
    pagetotal = {672},
    publisher = {Penguin Books},
    author = {Kurzweil, Ray},
    date = {2006-09-26},
}

@article{kuhl_machine_2020,
    title = {Machine Learning in Artificial Intelligence: Towards a Common Understanding},
    url = {http://arxiv.org/abs/2004.04686},
    shorttitle = {Machine Learning in Artificial Intelligence},
    abstract = {The application of "machine learning" and "artificial intelligence" has become popular within the last decade. Both terms are frequently used in science and media, sometimes interchangeably, sometimes with different meanings. In this work, we aim to clarify the relationship between these terms and, in particular, to specify the contribution of machine learning to artificial intelligence. We review relevant literature and present a conceptual framework which clarifies the role of machine learning to build (artificial) intelligent agents. Hence, we seek to provide more terminological clarity and a starting point for (interdisciplinary) discussions and future research.},
    journaltitle = {{arXiv}:2004.04686 [cs]},
    author = {Kühl, Niklas and Goutier, Marc and Hirt, Robin and Satzger, Gerhard},
    urldate = {2021-10-14},
    date = {2020-03-27},
    eprinttype = {arxiv},
    eprint = {2004.04686},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
    file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/UEIGXAC3/Kühl et al. - 2020 - Machine Learning in Artificial Intelligence Towar.pdf:application/pdf;arXiv.org Snapshot:/home/ck/Zotero/storage/TI7UL3FL/2004.html:text/html},
}

@article{mccarthy_what_nodate,
    title = {{What} {Is} {Artificial} {Intelligence}?},
    url = {http://jmc.stanford.edu/articles/whatisai/whatisai.pdf},
    abstract = {This article for the layman answers basic questions about artiﬁcial intelligence. The opinions expressed here are not all consensus opinion among researchers in {AI}.},    
    date = {2007-11-01},
    urldate = {2021-10-14},
    author = {{McCarthy}, John},
    langid = {english},
    file = {McCarthy - What is Artificial Intelligence.pdf:/home/ck/Zotero/storage/PV5NIE2K/McCarthy - What is Artificial Intelligence.pdf:application/pdf},
}


@book{russell_artificial_1995,
    location = {Englewood Cliffs, N.J},
    title = {Artificial intelligence: a modern approach},
    isbn = {978-0-13-103805-9},
    series = {Prentice Hall series in artificial intelligence},
    shorttitle = {Artificial intelligence},
    pagetotal = {932},
    publisher = {Prentice Hall},
    author = {Russell, Stuart J. and Norvig, Peter},
    date = {1995},
    langid = {english},
    keywords = {Artificial intelligence},
    file = {Russell and Norvig - 1995 - Artificial intelligence a modern approach.pdf:/home/ck/Zotero/storage/PHNBLII2/Russell and Norvig - 1995 - Artificial intelligence a modern approach.pdf:application/pdf},
}