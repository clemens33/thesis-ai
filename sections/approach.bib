
@inreference{noauthor_amazon_2021,
	title = {Amazon S3},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Amazon_S3&oldid=1057018762},
	abstract = {Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services ({AWS}) that provides object storage through a web service interface. Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its global e-commerce network. Amazon S3 can be employed to store any type of object, which allows for uses like storage for Internet applications, backup and recovery, disaster recovery, data archives, data lakes for analytics, and hybrid cloud storage.
{AWS} launched Amazon S3 in the United States on March 14, 2006, then in Europe in November 2007.},
	booktitle = {Wikipedia},
	urldate = {2021-12-29},
	date = {2021-11-24},
	langid = {english},
	note = {Page Version {ID}: 1057018762},
	file = {Snapshot:/home/ck/Zotero/storage/SIQQA83J/index.html:text/html},
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www. tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Martin~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Mane and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Viegas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@software{noauthor_entmax_2021,
	title = {entmax},
	rights = {{MIT}},
	url = {https://github.com/deep-spin/entmax},
	abstract = {The entmax mapping and its loss, a family of sparse softmax alternatives.},
	publisher = {{DeepSPIN}},
	urldate = {2021-11-22},
	date = {2021-11-16},
	note = {original-date: 2019-05-31T13:54:08Z},
}

@misc{krekel_pytest_2004,
	title = {pytest x.y},
	url = {https://github.com/pytest-dev/pytest},
	author = {Krekel, Holger and Oliveira, Bruno and Pfannschmidt, Ronny and Bruynooghe, Floris and Laugher, Brianna and Bruhin, Florian},
	date = {2004},
}

@online{kriechbaumer_master_nodate,
	title = {Master Thesis ({AI}) - {TabNet} Experiments},
	url = {https://mlflow.kriechbaumer.at/#/},
	author = {Kriechbaumer, Clemens},
	urldate = {2021-11-21},
	file = {MLflow:/home/ck/Zotero/storage/8AZPN6VB/mlflow.kriechbaumer.at.html:text/html},
}

@online{kriechbaumer_clemens33thesis-tex_nodate,
	title = {clemens33/thesis-tex: Master Thesis in {AI}},
	url = {https://github.com/clemens33/thesis-tex},
	shorttitle = {clemens33/thesis-tex},
	abstract = {Master Thesis in {AI}. Contribute to clemens33/thesis-tex development by creating an account on {GitHub}.},
	titleaddon = {{GitHub}},
	author = {Kriechbaumer, Clemens},
	urldate = {2021-11-21},
	langid = {english},
	file = {Snapshot:/home/ck/Zotero/storage/8AU7NMIG/thesis-tex.html:text/html},
}

@misc{pytestx.y,
  title =        {pytest x.y},
  author = {Krekel, Holger and Oliveira, Bruno and Pfannschmidt, Ronny and Bruynooghe, Floris and Laugher, Brianna and Bruhin, Florian},
  year =         {2004},
  url = {https://github.com/pytest-dev/pytest},
}


@article{loshchilov_decoupled_2019,
	title = {Decoupled Weight Decay Regularization},
	url = {http://arxiv.org/abs/1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in {TensorFlow} and {PyTorch}; the complete source code for our experiments is available at https://github.com/loshchil/{AdamW}-and-{SGDW}},
	journaltitle = {{arXiv}:1711.05101 [cs, math]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2021-11-21},
	date = {2019-01-04},
	eprinttype = {arxiv},
	eprint = {1711.05101},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@article{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2021-11-21},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ck/Zotero/storage/PFJPCXZS/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@online{noauthor_exploring_nodate,
	title = {Exploring Multi-Objective Hyperparameter Optimization},
	url = {https://blog.fastforwardlabs.com/2021/07/07/exploring-multi-objective -hyperparameter-optimization.html},
	abstract = {By Chris and Melanie.
The machine learning life cycle is more than data + model = {API}. We know there is a wealth of subtlety and finesse involved in data cleaning and feature engineering. In the same vein, there is more to model-building than feeding data in and reading off a prediction. {ML} model building requires thoughtfulness both in terms of which metric to optimize for a given problem, and how best to optimize your model for that metric!},
	urldate = {2021-11-21},
	langid = {english},
	file = {Snapshot:/home/ck/Zotero/storage/J4HDRZXN/exploring-multi-objective-hyperparameter-optimization.html:text/html},
}

@online{noauthor_ax_nodate,
	title = {Ax Â· Adaptive Experimentation Platform},
	url = {https://ax.dev//index.html},
	abstract = {Adaptive Experimentation Platform},
	urldate = {2021-11-21},
	file = {Snapshot:/home/ck/Zotero/storage/5GMSYMTZ/ax.dev.html:text/html},
}

@inproceedings{optuna_2019,
    title={Optuna: A Next-generation Hyperparameter Optimization Framework},
    author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    booktitle={Proceedings of the 25rd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
    year={2019}
}

@inproceedings{chen_xgboost_2016,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@software{clemens_kriechbaumer_clemens33mlflow_2021,
	title = {clemens33/mlflow},
	url = {https://github.com/clemens33/mlflow},
	abstract = {mlflow + mysql backend store setup using docker and docker-compose for simple self-hosted ml experiments tracking},
	author = {Clemens Kriechbaumer},
	urldate = {2021-11-20},
	date = {2021-08-12},
	note = {original-date: 2021-05-02T12:03:36Z},
	keywords = {machine-learning, mlflow, mlflow-docker, mlops, self-hosted},
}

@online{noauthor_mlflow_nodate,
	title = {{MLflow} - A platform for the machine learning lifecycle},
	url = {https://mlflow.org/},
	abstract = {An open source platform for the end-to-end machine learning lifecycle},
	titleaddon = {{MLflow}},
	urldate = {2021-11-20},
	langid = {english},
	file = {Snapshot:/home/ck/Zotero/storage/DVYT843N/mlflow.org.html:text/html},
}

@article{falcon2019pytorch,
  title={PyTorch Lightning},
  author={William {Falcon et al.}},
  journal={GitHub. Note: https://github.com /PyTorchLightning/pytorch-lightning},
  volume={3},
  year={2019}
}

@incollection{NEURIPS2019_9015,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle = {Advances in Neural Information Processing Systems 32},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {8024--8035},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}